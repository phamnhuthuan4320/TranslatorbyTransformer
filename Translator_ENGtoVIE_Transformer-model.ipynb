{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2874,"status":"ok","timestamp":1652510708104,"user":{"displayName":"Thuần Phạm","userId":"06674760698763323197"},"user_tz":-420},"id":"VBGIifTktlNl"},"outputs":[],"source":["import pathlib\n","import random\n","import string\n","import re\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1346,"status":"ok","timestamp":1652510709444,"user":{"displayName":"Thuần Phạm","userId":"06674760698763323197"},"user_tz":-420},"id":"Be3qdb3WtlNt"},"outputs":[],"source":["with open('/content/drive/MyDrive/LEARNING/NienLuan/Model/en_vi-dataset/vie.txt') as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","text_pairs = []\n","for line in lines:\n","    eng, vie = line.split(\"\\t\")\n","    vie = \"[start] \" + vie + \" [end]\"\n","    text_pairs.append((eng, vie))"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1652510709445,"user":{"displayName":"Thuần Phạm","userId":"06674760698763323197"},"user_tz":-420},"id":"qRSpRbGJwXCH","outputId":"5df91b79-2a83-4054-90a4-610c6b660393"},"outputs":[{"output_type":"stream","name":"stdout","text":["('The students learned many poems by heart.', '[start] Các sinh viên đã học thuộc lòng rất nhiều bài thơ. [end]')\n","(\"They haven't left.\", '[start] Họ chưa đi. [end]')\n","('It might be a good idea to get some sleep.', '[start] Có lẽ việc chợp mắt một lát là một ý tưởng không tồi. [end]')\n","('I think that Tom would be able to fix your computer.', '[start] Tôi nghĩ là Tom có thể sửa máy tính của bạn. [end]')\n","('I like both.', '[start] Tôi thích cả hai cái. [end]')\n"]}],"source":["for _ in range(5):\n","    print(random.choice(text_pairs))"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1652510709446,"user":{"displayName":"Thuần Phạm","userId":"06674760698763323197"},"user_tz":-420},"id":"TVX5U8RutlNx","outputId":"f4c3f519-4386-4671-a96d-2dbf1beb116e"},"outputs":[{"output_type":"stream","name":"stdout","text":["7558 total pairs\n","5292 training pairs\n","1133 validation pairs\n","1133 test pairs\n"]}],"source":["random.shuffle(text_pairs)\n","num_val_samples = int(0.15 * len(text_pairs))\n","num_train_samples = len(text_pairs) - 2 * num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n","test_pairs = text_pairs[num_train_samples + num_val_samples :]\n","\n","print(f\"{len(text_pairs)} total pairs\")\n","print(f\"{len(train_pairs)} training pairs\")\n","print(f\"{len(val_pairs)} validation pairs\")\n","print(f\"{len(test_pairs)} test pairs\")"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3550,"status":"ok","timestamp":1652510712990,"user":{"displayName":"Thuần Phạm","userId":"06674760698763323197"},"user_tz":-420},"id":"feGehoOltlN0"},"outputs":[],"source":["strip_chars = string.punctuation + \"¿\"\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","vocab_size = 15000\n","sequence_length = 20\n","batch_size = 64\n","\n","\n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n","\n","\n","eng_vectorization = TextVectorization(\n","    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",")\n","vie_vectorization = TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1,\n","    standardize=custom_standardization,\n",")\n","train_eng_texts = [pair[0] for pair in train_pairs]\n","train_vie_texts = [pair[1] for pair in train_pairs]\n","eng_vectorization.adapt(train_eng_texts)\n","vie_vectorization.adapt(train_vie_texts)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1652510712990,"user":{"displayName":"Thuần Phạm","userId":"06674760698763323197"},"user_tz":-420},"id":"fEbnU8pq0koT"},"outputs":[],"source":["\n","def format_dataset(eng, vie):\n","    eng = eng_vectorization(eng)\n","    vie = vie_vectorization(vie)\n","    return ({\"encoder_inputs\": eng, \"decoder_inputs\": vie[:, :-1],}, vie[:, 1:])\n","\n","\n","def make_dataset(pairs):\n","    eng_texts, vie_texts = zip(*pairs)\n","    eng_texts = list(eng_texts)\n","    vie_texts = list(vie_texts)\n","    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, vie_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset)\n","    return dataset.shuffle(2048).prefetch(16).cache()\n","\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1652510712991,"user":{"displayName":"Thuần Phạm","userId":"06674760698763323197"},"user_tz":-420},"id":"DxHVKsLg0koU","outputId":"5c442325-fb8e-40df-e579-366c5a2fafea"},"outputs":[{"output_type":"stream","name":"stdout","text":["inputs[\"encoder_inputs\"].shape: (64, 20)\n","inputs[\"decoder_inputs\"].shape: (64, 20)\n","targets.shape: (64, 20)\n"]}],"source":["for inputs, targets in train_ds.take(1):\n","    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n","    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n","    print(f\"targets.shape: {targets.shape}\")"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1652510712992,"user":{"displayName":"Thuần Phạm","userId":"06674760698763323197"},"user_tz":-420},"id":"5S9svTJC0koV"},"outputs":[],"source":["\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super(TransformerEncoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n","        attention_output = self.attention(\n","            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n","        )\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","\n","\n","class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n","        super(PositionalEmbedding, self).__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=vocab_size, output_dim=embed_dim\n","        )\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=embed_dim\n","        )\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","\n","class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n","        super(TransformerDecoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.latent_dim = latent_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","\n","        attention_output_1 = self.attention_1(\n","            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n","        )\n","        out_1 = self.layernorm_1(inputs + attention_output_1)\n","\n","        attention_output_2 = self.attention_2(\n","            query=out_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        out_2 = self.layernorm_2(out_1 + attention_output_2)\n","\n","        proj_output = self.dense_proj(out_2)\n","        return self.layernorm_3(out_2 + proj_output)\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n","            axis=0,\n","        )\n","        return tf.tile(mask, mult)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":980,"status":"ok","timestamp":1652510713964,"user":{"displayName":"Thuần Phạm","userId":"06674760698763323197"},"user_tz":-420},"id":"i1N3X6bX0koX"},"outputs":[],"source":["embed_dim = 256\n","latent_dim = 2048\n","num_heads = 8\n","\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n","encoder = keras.Model(encoder_inputs, encoder_outputs)\n","\n","decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n","encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n","x = layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n","\n","decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n","transformer = keras.Model(\n","    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",")"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":57868,"status":"ok","timestamp":1652510972550,"user":{"displayName":"Thuần Phạm","userId":"06674760698763323197"},"user_tz":-420},"id":"eL1zmciZ0koa","outputId":"bfc01417-e7bf-4d1c-d3f6-b379d9678f77"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"transformer\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n","                                                                                                  \n"," positional_embedding (Position  (None, None, 256)   3845120     ['encoder_inputs[0][0]']         \n"," alEmbedding)                                                                                     \n","                                                                                                  \n"," decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n","                                                                                                  \n"," transformer_encoder (Transform  (None, None, 256)   3155456     ['positional_embedding[0][0]']   \n"," erEncoder)                                                                                       \n","                                                                                                  \n"," model_1 (Functional)           (None, None, 15000)  12959640    ['decoder_inputs[0][0]',         \n","                                                                  'transformer_encoder[0][0]']    \n","                                                                                                  \n","==================================================================================================\n","Total params: 19,960,216\n","Trainable params: 19,960,216\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Epoch 1/30\n","83/83 [==============================] - 14s 76ms/step - loss: 2.8108 - accuracy: 0.2182 - val_loss: 2.3337 - val_accuracy: 0.3029\n","Epoch 2/30\n","83/83 [==============================] - 6s 68ms/step - loss: 2.1834 - accuracy: 0.3365 - val_loss: 2.0897 - val_accuracy: 0.3666\n","Epoch 3/30\n","83/83 [==============================] - 6s 69ms/step - loss: 1.8650 - accuracy: 0.4145 - val_loss: 1.7900 - val_accuracy: 0.4378\n","Epoch 4/30\n","83/83 [==============================] - 6s 68ms/step - loss: 1.6332 - accuracy: 0.4733 - val_loss: 1.6650 - val_accuracy: 0.4732\n","Epoch 5/30\n","83/83 [==============================] - 6s 68ms/step - loss: 1.4362 - accuracy: 0.5240 - val_loss: 1.7350 - val_accuracy: 0.4740\n","Epoch 6/30\n","83/83 [==============================] - 6s 68ms/step - loss: 1.2709 - accuracy: 0.5686 - val_loss: 1.5214 - val_accuracy: 0.5207\n","Epoch 7/30\n","83/83 [==============================] - 6s 69ms/step - loss: 1.1202 - accuracy: 0.6096 - val_loss: 1.5481 - val_accuracy: 0.5277\n","Epoch 8/30\n","83/83 [==============================] - 6s 68ms/step - loss: 0.9916 - accuracy: 0.6479 - val_loss: 1.4634 - val_accuracy: 0.5401\n","Epoch 9/30\n","83/83 [==============================] - 6s 69ms/step - loss: 0.8718 - accuracy: 0.6827 - val_loss: 1.4443 - val_accuracy: 0.5508\n","Epoch 10/30\n","83/83 [==============================] - 6s 68ms/step - loss: 0.7723 - accuracy: 0.7121 - val_loss: 1.4573 - val_accuracy: 0.5576\n","Epoch 11/30\n","83/83 [==============================] - 6s 69ms/step - loss: 0.6841 - accuracy: 0.7402 - val_loss: 1.4727 - val_accuracy: 0.5523\n","Epoch 12/30\n","83/83 [==============================] - 6s 68ms/step - loss: 0.6056 - accuracy: 0.7672 - val_loss: 1.4715 - val_accuracy: 0.5576\n","Epoch 13/30\n","83/83 [==============================] - 6s 69ms/step - loss: 0.5395 - accuracy: 0.7895 - val_loss: 1.5105 - val_accuracy: 0.5692\n","Epoch 14/30\n","83/83 [==============================] - 6s 69ms/step - loss: 0.4894 - accuracy: 0.8050 - val_loss: 1.5074 - val_accuracy: 0.5686\n","Epoch 15/30\n","83/83 [==============================] - 6s 69ms/step - loss: 0.4391 - accuracy: 0.8254 - val_loss: 1.4883 - val_accuracy: 0.5623\n","Epoch 16/30\n","83/83 [==============================] - 6s 70ms/step - loss: 0.3992 - accuracy: 0.8384 - val_loss: 1.5487 - val_accuracy: 0.5650\n","Epoch 17/30\n","83/83 [==============================] - 6s 69ms/step - loss: 0.3656 - accuracy: 0.8515 - val_loss: 1.5280 - val_accuracy: 0.5665\n","Epoch 18/30\n","83/83 [==============================] - 6s 69ms/step - loss: 0.3361 - accuracy: 0.8629 - val_loss: 1.5429 - val_accuracy: 0.5752\n","Epoch 19/30\n","83/83 [==============================] - 6s 70ms/step - loss: 0.3131 - accuracy: 0.8724 - val_loss: 1.5310 - val_accuracy: 0.5768\n","Epoch 20/30\n","83/83 [==============================] - 6s 70ms/step - loss: 0.2996 - accuracy: 0.8768 - val_loss: 1.5508 - val_accuracy: 0.5759\n","Epoch 21/30\n","83/83 [==============================] - 6s 71ms/step - loss: 0.2799 - accuracy: 0.8851 - val_loss: 1.5492 - val_accuracy: 0.5755\n","Epoch 22/30\n","83/83 [==============================] - 6s 70ms/step - loss: 0.2640 - accuracy: 0.8914 - val_loss: 1.5714 - val_accuracy: 0.5744\n","Epoch 23/30\n","83/83 [==============================] - 6s 71ms/step - loss: 0.2513 - accuracy: 0.8954 - val_loss: 1.5915 - val_accuracy: 0.5775\n","Epoch 24/30\n","83/83 [==============================] - 6s 71ms/step - loss: 0.2438 - accuracy: 0.8998 - val_loss: 1.5555 - val_accuracy: 0.5801\n","Epoch 25/30\n","83/83 [==============================] - 6s 71ms/step - loss: 0.2333 - accuracy: 0.9051 - val_loss: 1.6070 - val_accuracy: 0.5701\n","Epoch 26/30\n","83/83 [==============================] - 6s 72ms/step - loss: 0.2283 - accuracy: 0.9068 - val_loss: 1.6040 - val_accuracy: 0.5797\n","Epoch 27/30\n","83/83 [==============================] - 6s 72ms/step - loss: 0.2218 - accuracy: 0.9088 - val_loss: 1.6069 - val_accuracy: 0.5711\n","Epoch 28/30\n","83/83 [==============================] - 6s 72ms/step - loss: 0.2122 - accuracy: 0.9131 - val_loss: 1.6007 - val_accuracy: 0.5845\n","Epoch 29/30\n","83/83 [==============================] - 6s 71ms/step - loss: 0.2070 - accuracy: 0.9152 - val_loss: 1.6090 - val_accuracy: 0.5783\n","Epoch 30/30\n","83/83 [==============================] - 6s 72ms/step - loss: 0.2031 - accuracy: 0.9178 - val_loss: 1.6451 - val_accuracy: 0.5761\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f702e2b3f90>"]},"metadata":{},"execution_count":11}],"source":["epochs = 30\n","\n","transformer.summary()\n","transformer.compile(\n","    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",")\n","transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1700,"status":"ok","timestamp":1652510974245,"user":{"displayName":"Thuần Phạm","userId":"06674760698763323197"},"user_tz":-420},"id":"k_M1FaaW0kob","outputId":"d7c7ac60-200b-47d1-acc2-5485105d23dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["input_sentence: Tom tried to control his anger. ==> translated: [start] tom cố thuyết phục anh ta tới [end]\n","output_sentence: [start] Tom đã cố kiềm chế cơn giận của mình. [end]\n","***************************************************************************************************************************************************************************\n","input_sentence: Don't call the cops, man. ==> translated: [start] Đừng gọi cho nhóm lửa như thế nào [end]\n","output_sentence: [start] Đừng gọi cảnh sát, anh bạn. [end]\n","***************************************************************************************************************************************************************************\n","input_sentence: What you told me wasn't true. ==> translated: [start] bạn đã nói với tôi không [end]\n","output_sentence: [start] Những gì mày đã nói với tao không phải là sự thật. [end]\n","***************************************************************************************************************************************************************************\n","input_sentence: Can you make sense of this poem? ==> translated: [start] bạn có thể giải thích cái này không [end]\n","output_sentence: [start] Bạn có thể giải thích nghĩa của bài thơ này không? [end]\n","***************************************************************************************************************************************************************************\n","input_sentence: May I introduce my friend to you? ==> translated: [start] tôi có thể để bạn của tôi bạn có được không [end]\n","output_sentence: [start] Tớ giới thiệu bạn tớ với ấy nhé? [end]\n","***************************************************************************************************************************************************************************\n"]}],"source":["vie_vocab = vie_vectorization.get_vocabulary()\n","vie_index_lookup = dict(zip(range(len(vie_vocab)), vie_vocab))\n","max_decoded_sentence_length = 20\n","\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = eng_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = vie_vectorization([decoded_sentence])[:, :-1]\n","        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n","\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = vie_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","\n","\n","for _ in range(5):\n","    sentence = random.choice(test_pairs)\n","    input_sentence = sentence[0]\n","    output_sentence = sentence[1]\n","    translated = decode_sequence(input_sentence)\n","    print(\"input_sentence: \" + input_sentence + \" ==> translated: \" + translated)\n","    print(\"output_sentence: \" + output_sentence)\n","    print(\"***************************************************************************************************************************************************************************\")"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"ZaeWnMPX26PA","executionInfo":{"status":"ok","timestamp":1652516084325,"user_tz":-420,"elapsed":7775,"user":{"displayName":"Thuần Phạm","userId":"06674760698763323197"}},"outputId":"915c5a10-c87e-427f-9cdd-e288a359ade5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Nhập vào: this is my book\n"]},{"output_type":"execute_result","data":{"text/plain":["'[start] Đây là áo của tôi [end]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":42}],"source":["input_sentence = input('Nhập vào: ')\n","translated = decode_sequence(input_sentence)\n","translated"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Translator_ENGtoVIE_Transformer-model.ipynb","provenance":[],"mount_file_id":"1xwRODLHO_thkumMGmHfcckFyofDiKJhI","authorship_tag":"ABX9TyN3ETvQRuYqar7DIvsgQsLk"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}